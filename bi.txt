
## Perform the data classification algorithm using any Classification algorithm

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import datasets


from warnings import filterwarnings
filterwarnings(action='ignore')

df=pd.read_csv("diabetes.csv")
print(df)

print(df.shape)

print(df.describe())

#Checking for null values
print(df.isna().sum())
print(df.describe())

df.head()

from sklearn.linear_model import LogisticRegression 
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier

X = df.drop('Outcome', axis=1)
y = df['Outcome']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state= None)

X_train.head()

y_test.head()

y_test.head()

s = df.corr()
s

import seaborn as sns
sns.heatmap(s, annot = True, fmt = ' 0.1f' , linewidth = .5)

sns.countplot(x = 'Outcome', data = df , palette = ['g','r'])

sns.boxplot(data = df)



results = []

### Using LogisticRegression

model = LogisticRegression()
model.fit(X_train, y_train)
prediction = model.predict(X_test)
Accuracy = metrics.accuracy_score(prediction,y_test)
print('Accuracy:',metrics.accuracy_score(prediction,y_test))
results.append(['Logistic Regression', Accuracy])

#Confusion matrix
from sklearn.metrics import confusion_matrix,classification_report
confusion_mat = confusion_matrix(y_test,prediction)
print("Confusion matrix: \n",confusion_mat)
print(classification_report(y_test,prediction))

sns.heatmap(confusion_mat, annot = True)

### Using Support Vector

from sklearn.svm import SVC
model1 = SVC()
model1.fit(X_train, y_train)

y_pred = model1.predict(X_test)

from sklearn.metrics import accuracy_score
acc = accuracy_score(y_test,y_pred)
print(acc)
results.append(['Support Vector Machine', acc])

### Using KNN Neighbors

from sklearn.neighbors import KNeighborsClassifier
model2 = KNeighborsClassifier(n_neighbors=3)
model2.fit(X_train, y_train)
y_pred2 = model2.predict(X_test)

from sklearn.metrics import accuracy_score
accur = accuracy_score(y_test,y_pred2)
accur
results.append(['KNN Neighbors', accur])

### Using GaussianNB

from sklearn.naive_bayes import GaussianNB
model3 = GaussianNB()
model3.fit(X_train, y_train)
y_pred3 = model3.predict(X_test)

from sklearn.metrics import accuracy_score
xx = accuracy_score(y_test,y_pred3)
xx
results.append(['Gaussian Naive Bayes', xx])

### Using Decision Tree

from sklearn.tree import DecisionTreeClassifier
model4 = DecisionTreeClassifier(criterion='entropy',random_state=7)
model4.fit(X_train, y_train)
y_pred4 = model4.predict(X_test)

from sklearn.metrics import accuracy_score
yy = accuracy_score(y_test,y_pred4)
yy
results.append(['Decision Tree', yy])

# Creating DataFrame
results_df = pd.DataFrame(results, columns=['Model', 'Accuracy'])
results_df
-------------------------------------------------------------------------------------------

Implement K-Means clustering on Iris.csv dataset. Determine the number of clusters using the elbow method.

Dataset Link: https://www.kaggle.com/datasets/uciml/iris

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

data = pd.read_csv('Iris.csv')
data

# Extract the features (drop the species column)
features = data.drop('Species', axis=1)

# Standardize the feature values (optional but recommended)
scaler = StandardScaler()
data_scaled = scaler.fit_transform(features)

## Implementing K-Means Clustering

#Finding the optimum number of clusters for k-means classification
from sklearn.cluster import KMeans
wcss = []

for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, random_state = 0)
    kmeans.fit(data_scaled)
    wcss.append(kmeans.inertia_)

## Using the elbow method to determine the optimal number of clusters for k-means clustering

plt.plot(range(1, 11), wcss)
plt.title('The elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS') #within cluster sum of squares
plt.show()

# Plot the elbow curve to determine the optimal number of clusters
plt.figure(figsize=(8, 6))
plt.plot(range(1, 11), wcss, marker='o', linestyle='-', color='b')
plt.title('Elbow Method for Optimal K')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Inertia')
plt.grid(True)
plt.show()

## Implementing K-Means Clustering

kmeans = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)
y_kmeans = kmeans.fit_predict(data_scaled)

#Visualising the clusters
plt.scatter(data_scaled[y_kmeans == 0, 0], data_scaled[y_kmeans == 0, 1], s = 100, c = 'purple', label = 'Iris-setosa')
plt.scatter(data_scaled[y_kmeans == 1, 0], data_scaled[y_kmeans == 1, 1], s = 100, c = 'orange', label = 'Iris-versicolour')
plt.scatter(data_scaled[y_kmeans == 2, 0], data_scaled[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Iris-virginica')

#Plotting the centroids of the clusters
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 100, c = 'red', label = 'Centroids')

plt.legend()

# 3d scatterplot using matplotlib

fig = plt.figure(figsize = (15,15))
ax = fig.add_subplot(111, projection='3d')
plt.scatter(data_scaled[y_kmeans == 0, 0], data_scaled[y_kmeans == 0, 1], s = 100, c = 'purple', label = 'Iris-setosa')
plt.scatter(data_scaled[y_kmeans == 1, 0], data_scaled[y_kmeans == 1, 1], s = 100, c = 'lightyellow', label = 'Iris-versicolour')
plt.scatter(data_scaled[y_kmeans == 2, 0], data_scaled[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Iris-virginica')

#Plotting the centroids of the clusters
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 100, c = 'red', label = 'Centroids')
plt.show()
-------------------------------------------------------------------------------------------
Matrix multiplication

mapper

import sys

with open(sys.argv[1], 'r') as f:
    matrix_a = [list(map(int, line.strip().split())) for line in f]

with open(sys.argv[2], 'r') as f:
    matrix_b = [list(map(int, line.strip().split())) for line in f]

for i in range(len(matrix_a)):
    for j in range(len(matrix_b[0])):
        for k in range(len(matrix_b)):
            print(f"{i},{j}\t {matrix_a[i][k]},{matrix_b[k][j]}")
---------------------------------
reducer

import sys

current_key = None
current_value = []

for line in sys.stdin:
    key, value = line.strip().split('\t')
    i, j = key.split(',')

    if current_key is None:
        current_key = key

    if key != current_key:
        result = 0
        for value_pair in current_value:
            value1, value2 = value_pair.split(',')
            result += int(value1) * int(value2)
        print(f"{current_key}\t{result}")
        current_key = key
        current_value = []

    current_value.append(value)

result = 0
for value_pair in current_value:
    value1, value2 = value_pair.split(',')
    result += int(value1) * int(value2)
print(f"{current_key}\t{result}")
---------------------------------------------------------------------------------------
MongoDB

show databases

use cc
db.cc.insert({'name': xx})
db.cc.insert({'name': yy})
db.cc.insert({'name': zz})

db.cc.updateOne({'name': vv}, {$set:{'name': xx}})

db.cc.find()

db.cc.deleteOne({'name': dd})

db.cc.find()
show databases
db.dropDatabase()

show databases
------------------------------------------------------------------------------------


